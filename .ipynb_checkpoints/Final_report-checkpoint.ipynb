{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Estimating incoming web traffic is essential for tech giants to plan efficient server allocation and resource management. We present a low cost service which once trained can estimate page views and hence the traffic inflow at a given point of time. We have done our analysis and modeling on Wikipedia page view data. Our choice of using Long short-term memory (LSTM) based Recurrent Neural Nets to predict the page views has shown significant results. We were able to predict the peaks and troughs of page view graph with a high accuracy. \n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "For the tech giants, a big slice of the budget is used in managing server resources to handle the incoming web traffic. Server resources include(not limited to) the number of machines running, processing power, memory usage, etc. If resources are insufficient, users may experience latency. The users who are active during this time may even leave for similar services from rivals. On the other hand, if more resources than required are allocated, resources get underutilized, meaning more money than required is spent on the resources. This also has an adverse impact on the environment as keeping the resources powered needs a lot of energy. It is hence essential and prudent to have a low cost service which once trained can estimate page views and hence the traffic inflow at a given point of time. This will help in more efficient server allocation and resource management. We have addressed this problem of web traffic estimation of various Wikipedia pages using Long short-term memory (LSTM) based Recurrent Neural Nets with the time series data of Web Traffic as input. \n",
    "\n",
    "# 2. Related Work\n",
    "\n",
    "In the past Vector autoregression (VAR) models and Autoregressive integrated moving average (ARIMA) models have been used to model time series data. [3] does a comparative study of VAA model and univariate and multivariate ARIMA models. However these models were not able to capture dependencies of varying and unknown window size. Later RNNs were found to work better with time series data. An interesting blog on the effectiveness of RNNs can be found at [5] by Karpathy. However RNNs faced the problem of vanishing and exploding gradient. Later LSTM based RNNs were introduced to tackle all these problems and be able to capture long term dependencies in sequential data like time series data. LSTM was first introduced by Sepp Hochreiter and Jürgen Schmidhuber [2] in 1997. However they were refined and popularized by the works of many people including Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves. \n",
    "In our work we observed that dropout does not work well with RNNs as the recurrence in the network amplifies noise thus hurting learning which is in line with the claims of Bayer et al. [4]. Recently, many academic and non academic projects have used LSTMs as the core technology to analyse sequential data.  [6]-[9] talk about time series prediction using LSTM.\n",
    "\n",
    "# 3. Background\n",
    "\n",
    "Long Short Term Memory (LSTM) based Recurrent Neural Network (RNN) \n",
    "\n",
    "## 3.1 RNN\n",
    "\n",
    "RNNs are a neural network that was developed to model sequential data like time series data, video data, language etc because traditional neural networks like Convolutional Networks fail to encode or preserve sequential data. Recurrent Neural Networks are networks with loops that allow sequential information to persist. \n",
    "\n",
    "<img src=\"image/RNN-rolled.png\" style=\"height:80px\" title=\"Rolled Recurrent Neural Network [1]\"/>\n",
    "\n",
    "In the above figure, the network module A takes Xt , which is the input at time step t. It processes the information and generates the output ht . It also passes the output information to the next time step state of module A. So at the next time step, the output of the previous timesteps and the new input Xt+1 are considered to generate the output ht+1. It is hence a stateful network as opposed to a stateless network. This process can be visualised in the following figure.\n",
    "\n",
    "<img src=\"image/RNN-unrolled.png\" style=\"height:80px\" title=\"Unrolled Recurrent Neural Network [1]\"/>\n",
    "\n",
    "RNNs have been used in many applications with much success. However, RNNs suffer from a problem known as vanishing gradient problem, that effectively means that the output at a time step can only be affected by a certain number inputs at previous timesteps. This is a common problem in deep neural networks. This can also be thought of as the network cell state cannot capture long term dependencies. To mitigate this problem LSTMs were designed.\n",
    "\n",
    "## 3.2 LSTM\n",
    "\n",
    "LSTMs are special kind of RNNs. In these kind of RNNs, each module has more number parameters to be learned, but these parameters give the network more flexibility to decide what information to keep from previous time steps and what information to discard. This way LSTMs model long term dependencies. Following a diagrammatic representation of an LSTM module at different timesteps. \n",
    "\n",
    "<img src=\"image/LSTM3-chain.png\" style=\"height:200px\" title=\"The repeating module in an LSTM [1]\" />\n",
    "\n",
    "Where the following notation holds,\n",
    "\n",
    "<img src=\"image/LSTM2-notation.png\" style=\"height:50px\" title=\"Notation of the above figure [1]\" />\n",
    "\n",
    "The LSTM block above has 4 main components: a cell state, an input gate, an output gate and a forget gate. Each one of these components has a specific function. The cell state is supposed to remember values over arbitrary time intervals. The three gates can be considered as a conventional feedforward artificial neural network neuron. That is, they compute an activation of the weighted sum. More simply, they can be thought of as being involved in regulating the flow of values that pass through them.\n",
    "An LSTM works well to classify, process and predict time series data given time lags of varying size and duration between important events. This kind of Recurrent Neural Network is hence ideal for our problem because we have a series of values corresponding to the Web traffic data of the Wikipedia page per day and we want to predict the traffic data on upcoming days. This is a state of the art technique for identifying such relationships in time series data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dataset\n",
    "We have done our data exploration and analysis on Wikipedia page view data using the Rest API https://en.wikipedia.org/api/rest_v1/#/ . The API has an hourly limit of 200 hits per hour, so we were able to get 200 pages data per hour per machine. Since this is publicly available data showing only page view counts, there are no privacy concerns. The API returns a json response with the page name, granularity(daily in our case), timestamp and view count for the requested webpage. We parsed the json to extract required information. \n",
    "\n",
    "We have used Wikipedia web traffic data provided by Kaggle (https://www.kaggle.com/c/web-traffic-time-series-forecasting#evaluation) for our modelling.\n",
    "The data has been made publicly available by Kaggle and it shows only page title,host website (example en.wikipedia.com, zh.wikipedia.com etc), agent(spider, all etc) and view counts per day. \n",
    "\n",
    "All the data is from July 1, 2015 up until December 31, 2016.\n",
    "\n",
    "                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data exploration\n",
    "\n",
    "We have used Spark to collect and analyse the data. The code is completely parallelizable and can run on multiple cores for faster response. We created visualizations for a period of six months with daily samples. The visualizations have helped us understand which web pages have a periodic web traffic structure, which are harder to predict and what is the estimated value of web traffic at any given point of time. Using this analysis, we have identified a subset of topics like TV Series, refer figure below, whose traffic is more periodic in nature\n",
    "\n",
    "<img src=\"image/tv-series.png\" style=\"height:300px\" title=\"TV series data\" />\n",
    "\n",
    "as opposed to political figures like Donald Trump, where you can see a sudden spike in viewership right after the election results, refer figure below. \n",
    "\n",
    "<img src=\"image/POTUS.png\" style=\"height:300px\" title=\"Donald Trump\" />\n",
    "\n",
    "<b> All code can be found in the ‘web_traffic’ notebook. </b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Implementation Details\n",
    "\n",
    "We experimented with different libraries like native Tensorflow, Tflearn, Pytorch and Keras for our implementation. We decided to use Keras with Tensorflow backend for our final implementation. This decision was made because of the stability and ease of use of Keras. We implemented a single layer LSTM model followed by a dense layer to give the result of regression. \n",
    "\n",
    "While we spent hours tweaking the model parameters, we decided the below parameters for the final implementation.\n",
    "\n",
    "Sequence Length - 7 \n",
    "LSTM Memory Cells - 32\n",
    "LSTM Activation - ReLU\n",
    "Loss - MSE(mean_squared_error)\n",
    "\n",
    "We initialized our model weights to ‘Xavier’[11] initializer(aka ‘glorot_normal’) and then we used the ADAM optimizer [10] for converging the weights of the model during training. \n",
    "\n",
    "The input to our model is a time series with 7 time steps, each observation of the time series has four main components ‘Agent’(All or Spider), ‘Access’(Mobile, Desktop or All), ‘Source’(9 different sources like ‘en.wikipedia.org’, ‘de.wikipedia.org’, etc) and the ‘Web Traffic’ on this observation’s date. The input is further formatted into One-Hot Encoded Vectors for the categorical variables and left as real-valued for the ‘Web Traffic’. We combine the different One-Hot encodings to create a concatenated input of (1 x 7 x 15) which represents (number of datapoints x 7 time steps x (2 variables for Agent + 3 variables for Access + 9 variables for Source + 1 variable real-valued Web Traffic). The output is a real valued number indicating the number of pages visits corresponding to the page, agent,website on the next day. We trained the models on 10 epochs each.\n",
    "\n",
    "We have implemented two separate models, one for all data(145064 articles) and one for TV series data (538 articles). After converting the the data to its appropriate format, we get a full dataset of (78769752 x 7 x 15) and a smaller TV series dataset of (292134 x 7 x 15). The output is a corresponding real valued vector.\n",
    "\n",
    "The models were trained on two different machines \n",
    "\n",
    "ThinkPad 13 (Local) - Intel Core i5-6300 CPU with 8GB Memory running Ubuntu 16.04\n",
    "iLab Machine null.cs.rutgers.edu (Remote) - Intel Core i7-4770 with 16GB Memory and GeForce GT 630 GPU running CentOS 7.3\n",
    "\n",
    "The local machine was used for training on smaller datasets and tweaking parameters. The iLab machine was used to run the model with the full dataset. A gpu version of tensorflow was also installed to take advantage of the GPU available on the iLab machine.\n",
    "\n",
    "For the smaller models, it took anywhere between 20 mins to 3 hours to train. However, for the model on full dataset, it took ~55 hours to complete the training over 10 epochs.\n",
    "\n",
    "For training, we have split the dataset with first 80% values in training set and remaining 20% values in testing set. We also tested with shuffling the data and without shuffling the data for training. We achieved better results without shuffling the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Code\n",
    "\n",
    "We start by importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import sys\n",
    "from time import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing the Keras libraries and packages for LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Reading Dataset\n",
    "The dataset is read from the training file, full data is stored in `train` and TV series data is filtered and stored in `train_TV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>2015-07-01</th>\n",
       "      <th>2015-07-02</th>\n",
       "      <th>2015-07-03</th>\n",
       "      <th>2015-07-04</th>\n",
       "      <th>2015-07-05</th>\n",
       "      <th>2015-07-06</th>\n",
       "      <th>2015-07-07</th>\n",
       "      <th>2015-07-08</th>\n",
       "      <th>2015-07-09</th>\n",
       "      <th>...</th>\n",
       "      <th>2016-12-22</th>\n",
       "      <th>2016-12-23</th>\n",
       "      <th>2016-12-24</th>\n",
       "      <th>2016-12-25</th>\n",
       "      <th>2016-12-26</th>\n",
       "      <th>2016-12-27</th>\n",
       "      <th>2016-12-28</th>\n",
       "      <th>2016-12-29</th>\n",
       "      <th>2016-12-30</th>\n",
       "      <th>2016-12-31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2NE1_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2PM_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3C_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4minute_zh.wikipedia.org_all-access_spider</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Page  2015-07-01  2015-07-02  \\\n",
       "0            2NE1_zh.wikipedia.org_all-access_spider        18.0        11.0   \n",
       "1             2PM_zh.wikipedia.org_all-access_spider        11.0        14.0   \n",
       "2              3C_zh.wikipedia.org_all-access_spider         1.0         0.0   \n",
       "3         4minute_zh.wikipedia.org_all-access_spider        35.0        13.0   \n",
       "4  52_Hz_I_Love_You_zh.wikipedia.org_all-access_s...         0.0         0.0   \n",
       "\n",
       "   2015-07-03  2015-07-04  2015-07-05  2015-07-06  2015-07-07  2015-07-08  \\\n",
       "0         5.0        13.0        14.0         9.0         9.0        22.0   \n",
       "1        15.0        18.0        11.0        13.0        22.0        11.0   \n",
       "2         1.0         1.0         0.0         4.0         0.0         3.0   \n",
       "3        10.0        94.0         4.0        26.0        14.0         9.0   \n",
       "4         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   2015-07-09     ...      2016-12-22  2016-12-23  2016-12-24  2016-12-25  \\\n",
       "0        26.0     ...            32.0        63.0        15.0        26.0   \n",
       "1        10.0     ...            17.0        42.0        28.0        15.0   \n",
       "2         4.0     ...             3.0         1.0         1.0         7.0   \n",
       "3        11.0     ...            32.0        10.0        26.0        27.0   \n",
       "4         0.0     ...            48.0         9.0        25.0        13.0   \n",
       "\n",
       "   2016-12-26  2016-12-27  2016-12-28  2016-12-29  2016-12-30  2016-12-31  \n",
       "0        14.0        20.0        22.0        19.0        18.0        20.0  \n",
       "1         9.0        30.0        52.0        45.0        26.0        20.0  \n",
       "2         4.0         4.0         6.0         3.0         4.0        17.0  \n",
       "3        16.0        11.0        17.0        19.0        10.0        11.0  \n",
       "4         3.0        11.0        27.0        13.0        36.0        10.0  \n",
       "\n",
       "[5 rows x 551 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing dataset\n",
    "train = pd.read_csv('./data/train_1.csv').fillna(0)\n",
    "page = train['Page']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>2015-07-01</th>\n",
       "      <th>2015-07-02</th>\n",
       "      <th>2015-07-03</th>\n",
       "      <th>2015-07-04</th>\n",
       "      <th>2015-07-05</th>\n",
       "      <th>2015-07-06</th>\n",
       "      <th>2015-07-07</th>\n",
       "      <th>2015-07-08</th>\n",
       "      <th>2015-07-09</th>\n",
       "      <th>...</th>\n",
       "      <th>2016-12-22</th>\n",
       "      <th>2016-12-23</th>\n",
       "      <th>2016-12-24</th>\n",
       "      <th>2016-12-25</th>\n",
       "      <th>2016-12-26</th>\n",
       "      <th>2016-12-27</th>\n",
       "      <th>2016-12-28</th>\n",
       "      <th>2016-12-29</th>\n",
       "      <th>2016-12-30</th>\n",
       "      <th>2016-12-31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8457</th>\n",
       "      <td>A_Series_of_Unfortunate_Events_(TV_series)_en....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>2279.0</td>\n",
       "      <td>2564.0</td>\n",
       "      <td>2192.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>1614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>American_Crime_(TV_series)_en.wikipedia.org_de...</td>\n",
       "      <td>876.0</td>\n",
       "      <td>663.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>727.0</td>\n",
       "      <td>947.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>736.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1258.0</td>\n",
       "      <td>1253.0</td>\n",
       "      <td>1352.0</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>1082.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8538</th>\n",
       "      <td>Animal_Kingdom_(TV_series)_en.wikipedia.org_de...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>706.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>458.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>2496.0</td>\n",
       "      <td>2832.0</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>871.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8567</th>\n",
       "      <td>Arrow_(TV_series)_en.wikipedia.org_desktop_all...</td>\n",
       "      <td>8233.0</td>\n",
       "      <td>7920.0</td>\n",
       "      <td>7049.0</td>\n",
       "      <td>6736.0</td>\n",
       "      <td>8129.0</td>\n",
       "      <td>8946.0</td>\n",
       "      <td>8638.0</td>\n",
       "      <td>8529.0</td>\n",
       "      <td>8812.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6482.0</td>\n",
       "      <td>5263.0</td>\n",
       "      <td>4435.0</td>\n",
       "      <td>4269.0</td>\n",
       "      <td>5299.0</td>\n",
       "      <td>6665.0</td>\n",
       "      <td>6684.0</td>\n",
       "      <td>6538.0</td>\n",
       "      <td>5870.0</td>\n",
       "      <td>4593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8621</th>\n",
       "      <td>Bates_Motel_(TV_series)_en.wikipedia.org_deskt...</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>2088.0</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>2079.0</td>\n",
       "      <td>2632.0</td>\n",
       "      <td>2266.0</td>\n",
       "      <td>2368.0</td>\n",
       "      <td>2171.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>1335.0</td>\n",
       "      <td>1757.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>1568.0</td>\n",
       "      <td>1338.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 551 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Page  2015-07-01  \\\n",
       "8457  A_Series_of_Unfortunate_Events_(TV_series)_en....         0.0   \n",
       "8515  American_Crime_(TV_series)_en.wikipedia.org_de...       876.0   \n",
       "8538  Animal_Kingdom_(TV_series)_en.wikipedia.org_de...         0.0   \n",
       "8567  Arrow_(TV_series)_en.wikipedia.org_desktop_all...      8233.0   \n",
       "8621  Bates_Motel_(TV_series)_en.wikipedia.org_deskt...      2025.0   \n",
       "\n",
       "      2015-07-02  2015-07-03  2015-07-04  2015-07-05  2015-07-06  2015-07-07  \\\n",
       "8457         0.0         0.0         0.0         5.0        30.0         5.0   \n",
       "8515       663.0       671.0       660.0       727.0       947.0       739.0   \n",
       "8538         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "8567      7920.0      7049.0      6736.0      8129.0      8946.0      8638.0   \n",
       "8621      1953.0      2088.0      1969.0      2079.0      2632.0      2266.0   \n",
       "\n",
       "      2015-07-08  2015-07-09     ...      2016-12-22  2016-12-23  2016-12-24  \\\n",
       "8457         3.0         2.0     ...          1853.0      2023.0      1767.0   \n",
       "8515       749.0       736.0     ...          1170.0       953.0       836.0   \n",
       "8538         0.0         0.0     ...           706.0       599.0       607.0   \n",
       "8567      8529.0      8812.0     ...          6482.0      5263.0      4435.0   \n",
       "8621      2368.0      2171.0     ...          1496.0      1199.0      1113.0   \n",
       "\n",
       "      2016-12-25  2016-12-26  2016-12-27  2016-12-28  2016-12-29  2016-12-30  \\\n",
       "8457      1525.0      1959.0      2279.0      2564.0      2192.0      1959.0   \n",
       "8515       728.0      1078.0      1258.0      1253.0      1352.0      1178.0   \n",
       "8538       458.0       741.0      1108.0      2496.0      2832.0      1141.0   \n",
       "8567      4269.0      5299.0      6665.0      6684.0      6538.0      5870.0   \n",
       "8621      1022.0      1335.0      1757.0      1690.0      1609.0      1568.0   \n",
       "\n",
       "      2016-12-31  \n",
       "8457      1614.0  \n",
       "8515      1082.0  \n",
       "8538       871.0  \n",
       "8567      4593.0  \n",
       "8621      1338.0  \n",
       "\n",
       "[5 rows x 551 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_TV = train[train['Page'].str.contains('TV_series')]\n",
    "train_TV.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the features\n",
    "The features are extracted from the Page name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_source(page):\n",
    "    res = re.search('_[a-z]+\\.[a-z]+\\.[a-z]+_[a-z\\-]+_[a-z]+',page)\n",
    "    if res:\n",
    "        return res.group().split('_')[1]\n",
    "    return 'na'\n",
    "\n",
    "def get_access(page):\n",
    "    res = re.search('_[a-z]+\\.[a-z]+\\.[a-z]+_[a-z\\-]+_[a-z]+',page)\n",
    "    if res:\n",
    "        return res.group().split('_')[2]\n",
    "    return 'na'\n",
    "\n",
    "def get_agent(page):\n",
    "    res = re.search('_[a-z]+\\.[a-z]+\\.[a-z]+_[a-z\\-]+_[a-z]+',page)\n",
    "    if res:\n",
    "        return res.group().split('_')[3]\n",
    "    return 'na'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate code for full dataset and tv series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = train.Page.map(get_source)\n",
    "access = train.Page.map(get_access)\n",
    "agent = train.Page.map(get_agent)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "source = le.fit_transform(source)\n",
    "access = le.fit_transform(access)\n",
    "agent = le.fit_transform(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_TV = train_TV.Page.map(get_source)\n",
    "access_TV = train_TV.Page.map(get_access)\n",
    "agent_TV = train_TV.Page.map(get_agent)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "source_TV = le.fit_transform(source_TV)\n",
    "access_TV = le.fit_transform(access_TV)\n",
    "agent_TV = le.fit_transform(agent_TV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the Page column from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop('Page',axis=1)\n",
    "train_TV = train_TV.drop('Page', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "One-hot encoding the categories, of course separately for TV series and full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = np.reshape(source, (-1,1))\n",
    "access = np.reshape(access, (-1,1))\n",
    "agent = np.reshape(agent, (-1,1))\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "source = enc.fit_transform(source)\n",
    "access = enc.fit_transform(access)\n",
    "agent = enc.fit_transform(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_TV = np.reshape(source_TV, (-1,1))\n",
    "access_TV = np.reshape(access_TV, (-1,1))\n",
    "agent_TV = np.reshape(agent_TV, (-1,1))\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "source_TV = enc.fit_transform(source_TV)\n",
    "access_TV = enc.fit_transform(access_TV)\n",
    "agent_TV = enc.fit_transform(agent_TV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that we're on right track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145063, 9) (145063, 550) (538, 1) (538, 550)\n"
     ]
    }
   ],
   "source": [
    "print source.shape, train.shape, source_TV.shape, train_TV.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating all the data (one-hot encodings and real-values) into one monolithic data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 15, 550) 314\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "## Memory issues if we select the whole training data\n",
    "## Please refer to the the attached python code to see the implementation using generator\n",
    "# for article in range(len(train)):\n",
    "for article in range(500):\n",
    "    row = train.iloc[article,:].values\n",
    "    row_len = len(row)\n",
    "    row = np.reshape(row,(1,row_len))\n",
    "    sourceCol = np.rot90(np.repeat(source[article,:].toarray(), row_len, axis=0),1,(0,1))\n",
    "    accessCol = np.rot90(np.repeat(access[article,:].toarray(), row_len, axis=0),1,(0,1))\n",
    "    agentCol = np.rot90(np.repeat(agent[article,:].toarray(), row_len, axis=0),1,(0,1))\n",
    "    row = np.append(row, sourceCol, axis=0)\n",
    "    row = np.append(row, accessCol, axis=0)\n",
    "    row = np.append(row, agentCol, axis=0)\n",
    "    rows.append(row)\n",
    "\n",
    "rows = np.array(rows)    \n",
    "print rows.shape, sys.getsizeof(rows)/(1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows_TV = []\n",
    "\n",
    "for article in range(len(train_TV)):\n",
    "    row = train_TV.iloc[article,:].values\n",
    "    row_len = len(row)\n",
    "    row = np.reshape(row,(1,row_len))\n",
    "    sourceCol = np.rot90(np.repeat(source_TV[article,:].toarray(), row_len, axis=0),1,(0,1))\n",
    "    accessCol = np.rot90(np.repeat(access_TV[article,:].toarray(), row_len, axis=0),1,(0,1))\n",
    "    agentCol = np.rot90(np.repeat(agent_TV[article,:].toarray(), row_len, axis=0),1,(0,1))\n",
    "    row = np.append(row, sourceCol, axis=0)\n",
    "    row = np.append(row, accessCol, axis=0)\n",
    "    row = np.append(row, agentCol, axis=0)\n",
    "    rows_TV.append(row)\n",
    "\n",
    "rows_TV = np.array(rows_TV)    \n",
    "print rows.shape, sys.getsizeof(rows_TV)/(1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to time-series data\n",
    "The time series data with 7 sequence length. We can try out different `seq_length` by changing this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 7\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "## Refer to above comment - This is a sample -Check generator code in python files attached\n",
    "# for article in range(len(train)):\n",
    "for article in range(5000):\n",
    "    for i in range(0, len(rows[article,0]) - seq_length, 1):\n",
    "        ts_data = rows[article, :, i: i + seq_length]\n",
    "        ts_data = np.rot90(ts_data, 3, (0,1))\n",
    "        ts_output = rows[article, 0, i + seq_length]\n",
    "        X.append(ts_data)\n",
    "        y.append(ts_output)\n",
    "\n",
    "    \n",
    "X = np.array(X)\n",
    "print X.shape, len(y), sys.getsizeof(X)/(1024*1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 7\n",
    "\n",
    "X_TV = []\n",
    "y_TV = []\n",
    "\n",
    "for article in range(len(train_TV)):\n",
    "    for i in range(0, len(rows[article,0]) - seq_length, 1):\n",
    "        ts_data = rows[article, :, i: i + seq_length]\n",
    "        ts_data = np.rot90(ts_data, 3, (0,1))\n",
    "        ts_output = rows[article, 0, i + seq_length]\n",
    "        X_TV.append(ts_data)\n",
    "        y_TV.append(ts_output)\n",
    "\n",
    "    \n",
    "X_TV = np.array(X_TV)\n",
    "print X_TV.shape, len(y_TV), sys.getsizeof(X_TV)/(1024*1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train Split\n",
    "Using the `train_test_split` function to split the data into training and testing datasets. For both the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle=True)\n",
    "\n",
    "y_train = np.reshape(y_train,(-1,1))\n",
    "y_test = np.reshape(y_test, (-1,1))\n",
    "\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_TV_train, X_TV_test, y_TV_train, y_TV_test = train_test_split(X_TV, y_TV, test_size = 0.2, shuffle=False)\n",
    "\n",
    "y_TV_train = np.reshape(y_TV_train,(-1,1))\n",
    "y_TV_test = np.reshape(y_TV_test, (-1,1))\n",
    "\n",
    "print X_TV_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Training\n",
    "The LSTM model is used for this dataset. An LSTM layer is followed by a Fully Connected layer. MAPE is used as a loss metric and Adam optimizer is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the input layerand the LSTM layer\n",
    "regressor.add(LSTM(units = 32, activation = 'relu', input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "history = regressor.fit(X_train, y_train, validation_split=0.33, batch_size = 32, epochs = 10, verbose = 1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor_TV = Sequential()\n",
    "\n",
    "# Adding the input layerand the LSTM layer\n",
    "regressor_TV.add(LSTM(units = 32, activation = 'relu', input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "regressor_TV.add(Dense(units = 1))\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs_TV/{}\".format(time()))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor_TV.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "history_TV = regressor_TV.fit(X_TV_train, y_TV_train, validation_split=0.33, batch_size = 32, epochs = 10, verbose = 1, callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor.save('model_500a_7ts_32u_lstm_adam_32b_10e.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor_TV.save('model_tv_7ts_32u_lstm_adam_32b_10e.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Analysis and Results\n",
    "<table style=\"width:33%;border: 1px solid black;border-collapse: collapse;\">\n",
    "\n",
    "  <tr>\n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\"></th>\n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\">Training Loss</th> \n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\">Testing Loss</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\">Complete Data</th>\n",
    "    <td style=\"border: 1px solid black;border-collapse: collapse;\"></td>\n",
    "    <td style=\"border: 1px solid black;border-collapse: collapse;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\">TV Series Data</th>\n",
    "    <td style=\"border: 1px solid black;border-collapse: collapse;\"></td>\n",
    "    <td style=\"border: 1px solid black;border-collapse: collapse;\"></td>\n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Training/Validation Loss \n",
    "We see how the models' losses fared during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history_TV.history['loss'])\n",
    "plt.plot(history_TV.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We evaluate the model visually as well as numerically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Numerical Evaluation\n",
    "We evaluate the loss on testing data and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_train = regressor.evaluate(X_train, y_train,batch_size=32,verbose=0)\n",
    "loss_test = regressor.evaluate(X_test, y_test,batch_size=32,verbose=0)\n",
    "\n",
    "print 'Training Loss: ', loss_train\n",
    "print 'Testing Loss: ', loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_train = regressor_TV.evaluate(X_TV_train, y_TV_train,batch_size=32,verbose=1)\n",
    "loss_test = regressor_TV.evaluate(X_TV_test, y_TV_test,batch_size=32,verbose=1)\n",
    "\n",
    "print 'Training Loss: ', loss_train\n",
    "print 'Testing Loss: ', loss_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual\n",
    "We plot the real data against the predicted data for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = X_test\n",
    "\n",
    "# Getting the predicted Web View\n",
    "y_pred = regressor.predict(inputs)\n",
    "\n",
    "print X_test.shape, len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = X_TV_test\n",
    "\n",
    "# Getting the predicted Web View\n",
    "y_TV_pred = regressor_TV.predict(inputs)\n",
    "\n",
    "print X_TV_test.shape, len(y_TV_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualising Result\n",
    "plt.figure\n",
    "plt.plot(y_test[1200:1300], color = 'red', label = 'Real Web View')\n",
    "plt.plot(y_pred[1200:1300], color = 'blue', label = 'Predicted Web View')\n",
    "plt.title('Web View Forecasting')\n",
    "plt.xlabel('Number of Days from Start')\n",
    "plt.ylabel('Web View')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(y_test[1200:1220], color = 'red', label = 'Real Web View')\n",
    "plt.plot(y_pred[1200:1220], color = 'blue', label = 'Predicted Web View')\n",
    "plt.title('Web View Forecasting')\n",
    "plt.xlabel('Number of Days from Start')\n",
    "plt.ylabel('Web View')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualising Result\n",
    "plt.figure\n",
    "plt.plot(y_TV_test[1200:1300], color = 'red', label = 'Real Web View')\n",
    "plt.plot(y_TV_pred[1200:1300], color = 'blue', label = 'Predicted Web View')\n",
    "plt.title('Web View Forecasting')\n",
    "plt.xlabel('Number of Days from Start')\n",
    "plt.ylabel('Web View')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(y_TV_test[1200:1220], color = 'red', label = 'Real Web View')\n",
    "plt.plot(y_TV_pred[1200:1220], color = 'blue', label = 'Predicted Web View')\n",
    "plt.title('Web View Forecasting')\n",
    "plt.xlabel('Number of Days from Start')\n",
    "plt.ylabel('Web View')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model\n",
    "The code below can be used to create a generative model. Given a random seed, the same model can be used to generate the Web Traffic prediction for next 100 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = np.random.randint(0, len(X_TV_test)-1)\n",
    "pattern = X_TV_test[start,:,:]\n",
    "y_TV_gen = []\n",
    "gen_len = 100\n",
    "\n",
    "for i in range(gen_len):\n",
    "    x = np.reshape(pattern, (1, 7, 15))\n",
    "    prediction = regressor_TV.predict(x)\n",
    "    temp_x = x[:,:,14]\n",
    "    temp_x = [[temp_x[0][i+1] for i in range(len(temp_x[0])-1)]]\n",
    "    temp_x[0].append(prediction[0][0])\n",
    "    x[:,:,14] = temp_x\n",
    "    pattern = np.array(x)\n",
    "    y_TV_gen.append(prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure\n",
    "plt.plot(y_TV_test[start:start+gen_len], color = 'red', label = 'Real Web View')\n",
    "plt.plot(y_TV_gen, color = 'blue', label = 'Predicted Web View')\n",
    "plt.title('Web View Forecasting')\n",
    "plt.xlabel('Number of Days from Start')\n",
    "plt.ylabel('Web View')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Acknowledgements\n",
    "We would like to thank Google Brain team and the open source community for creating TensorFlow. We would also like to thank François Chollet, Google, Microsoft and all other contributors for creating Keras. We have used both these libraries in our work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
