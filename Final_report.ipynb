{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Estimating incoming web traffic is essential for tech giants to plan efficient server allocation and resource management. We present a low cost service which once trained can estimate page views and hence the traffic inflow at a given point of time. We have done our analysis and modeling on Wikipedia page view data. Our choice of using Long short-term memory (LSTM) based Recurrent Neural Nets to predict the page views has shown significant results. We were able to predict the peaks and troughs of page view graph with a high accuracy. \n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "For the tech giants, a big slice of the budget is used in managing server resources to handle the incoming web traffic. Server resources include(not limited to) the number of machines running, processing power, memory usage, etc. If resources are insufficient, users may experience latency. The users who are active during this time may even leave for similar services from rivals. On the other hand, if more resources than required are allocated, resources get underutilized, meaning more money than required is spent on the resources. This also has an adverse impact on the environment as keeping the resources powered needs a lot of energy. It is hence essential and prudent to have a low cost service which once trained can estimate page views and hence the traffic inflow at a given point of time. This will help in more efficient server allocation and resource management. We have addressed this problem of web traffic estimation of various Wikipedia pages using Long short-term memory (LSTM) based Recurrent Neural Nets with the time series data of Web Traffic as input. \n",
    "\n",
    "# 2. Related Work\n",
    "\n",
    "In the past Vector autoregression (VAR) models and Autoregressive integrated moving average (ARIMA) models have been used to model time series data. [3] does a comparative study of VAA model and univariate and multivariate ARIMA models. However these models were not able to capture dependencies of varying and unknown window size. Later RNNs were found to work better with time series data. An interesting blog on the effectiveness of RNNs can be found at [5] by Karpathy. However RNNs faced the problem of vanishing and exploding gradient. Later LSTM based RNNs were introduced to tackle all these problems and be able to capture long term dependencies in sequential data like time series data. LSTM was first introduced by Sepp Hochreiter and Jürgen Schmidhuber [2] in 1997. However they were refined and popularized by the works of many people including Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves. \n",
    "In our work we observed that dropout does not work well with RNNs as the recurrence in the network amplifies noise thus hurting learning which is in line with the claims of Bayer et al. [4]. Recently, many academic and non academic projects have used LSTMs as the core technology to analyse sequential data.  [6]-[9] talk about time series prediction using LSTM.\n",
    "\n",
    "# 3. Background\n",
    "\n",
    "Long Short Term Memory (LSTM) based Recurrent Neural Network (RNN) \n",
    "\n",
    "## 3.1 RNN\n",
    "\n",
    "RNNs are a neural network that was developed to model sequential data like time series data, video data, language etc because traditional neural networks like Convolutional Networks fail to encode or preserve sequential data. Recurrent Neural Networks are networks with loops that allow sequential information to persist. \n",
    "\n",
    "<img src=\"image/RNN-rolled.png\" style=\"height:80px\" title=\"Rolled Recurrent Neural Network [1]\"/>\n",
    "\n",
    "In the above figure, the network module A takes Xt , which is the input at time step t. It processes the information and generates the output ht . It also passes the output information to the next time step state of module A. So at the next time step, the output of the previous timesteps and the new input Xt+1 are considered to generate the output ht+1. It is hence a stateful network as opposed to a stateless network. This process can be visualised in the following figure.\n",
    "\n",
    "<img src=\"image/RNN-unrolled.png\" style=\"height:80px\" title=\"Unrolled Recurrent Neural Network [1]\"/>\n",
    "\n",
    "RNNs have been used in many applications with much success. However, RNNs suffer from a problem known as vanishing gradient problem, that effectively means that the output at a time step can only be affected by a certain number inputs at previous timesteps. This is a common problem in deep neural networks. This can also be thought of as the network cell state cannot capture long term dependencies. To mitigate this problem LSTMs were designed.\n",
    "\n",
    "## 3.2 LSTM\n",
    "\n",
    "LSTMs are special kind of RNNs. In these kind of RNNs, each module has more number parameters to be learned, but these parameters give the network more flexibility to decide what information to keep from previous time steps and what information to discard. This way LSTMs model long term dependencies. Following a diagrammatic representation of an LSTM module at different timesteps. \n",
    "\n",
    "<img src=\"image/LSTM3-chain.png\" style=\"height:200px\" title=\"The repeating module in an LSTM [1]\" />\n",
    "\n",
    "Where the following notation holds,\n",
    "\n",
    "<img src=\"image/LSTM2-notation.png\" style=\"height:50px\" title=\"Notation of the above figure [1]\" />\n",
    "\n",
    "The LSTM block above has 4 main components: a cell state, an input gate, an output gate and a forget gate. Each one of these components has a specific function. The cell state is supposed to remember values over arbitrary time intervals. The three gates can be considered as a conventional feedforward artificial neural network neuron. That is, they compute an activation of the weighted sum. More simply, they can be thought of as being involved in regulating the flow of values that pass through them.\n",
    "An LSTM works well to classify, process and predict time series data given time lags of varying size and duration between important events. This kind of Recurrent Neural Network is hence ideal for our problem because we have a series of values corresponding to the Web traffic data of the Wikipedia page per day and we want to predict the traffic data on upcoming days. This is a state of the art technique for identifying such relationships in time series data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dataset\n",
    "We have done our data exploration and analysis on Wikipedia page view data using the Rest API https://en.wikipedia.org/api/rest_v1/#/ . The API has an hourly limit of 200 hits per hour, so we were able to get 200 pages data per hour per machine. Since this is publicly available data showing only page view counts, there are no privacy concerns. The API returns a json response with the page name, granularity(daily in our case), timestamp and view count for the requested webpage. We parsed the json to extract required information. \n",
    "\n",
    "We have used Wikipedia web traffic data provided by Kaggle (https://www.kaggle.com/c/web-traffic-time-series-forecasting#evaluation) for our modelling.\n",
    "The data has been made publicly available by Kaggle and it shows only page title,host website (example en.wikipedia.com, zh.wikipedia.com etc), agent(spider, all etc) and view counts per day. \n",
    "\n",
    "All the data is from July 1, 2015 up until December 31, 2016.\n",
    "\n",
    "                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data exploration\n",
    "\n",
    "We have used Spark to collect and analyse the data. The code is completely parallelizable and can run on multiple cores for faster response. We created visualizations for a period of six months with daily samples. The visualizations have helped us understand which web pages have a periodic web traffic structure, which are harder to predict and what is the estimated value of web traffic at any given point of time. Using this analysis, we have identified a subset of topics like TV Series, refer figure below, whose traffic is more periodic in nature\n",
    "\n",
    "<img src=\"image/tv-series.png\" style=\"height:300px\" title=\"TV series data\" />\n",
    "\n",
    "as opposed to political figures like Donald Trump, where you can see a sudden spike in viewership right after the election results, refer figure below. \n",
    "\n",
    "<img src=\"image/POTUS.png\" style=\"height:300px\" title=\"Donald Trump\" />\n",
    "\n",
    "<b> All code can be found in the ‘web_traffic’ notebook. </b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Analysis and Results\n",
    "<table style=\"width:33%;border: 1px solid black;border-collapse: collapse;\">\n",
    "\n",
    "  <tr>\n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\"></th>\n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\">Training Loss</th> \n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\">Testing Loss</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\">Complete Data</th>\n",
    "    <td style=\"border: 1px solid black;border-collapse: collapse;\"></td>\n",
    "    <td style=\"border: 1px solid black;border-collapse: collapse;\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style=\"border: 1px solid black;border-collapse: collapse;\">TV Series Data</th>\n",
    "    <td style=\"border: 1px solid black;border-collapse: collapse;\"></td>\n",
    "    <td style=\"border: 1px solid black;border-collapse: collapse;\"></td>\n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Acknowledgements\n",
    "We would like to thank Google Brain team and the open source community for creating TensorFlow. We would also like to thank François Chollet, Google, Microsoft and all other contributors for creating Keras. We have used both these libraries in our work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
